# Configuration for Rasa NLU.
# https://rasa.com/docs/rasa/nlu/components/
language: en

# pipeline:
#   - name: HFTransformersNLP
#     # Name of the language model to use
#     model_name: "bert"
#     # Pre-Trained weights to be loaded
#     model_weights: "bert-base-uncased"
#     # An optional path to a specific directory to download and cache the pre-trained model weights.
#     # The `default` cache_dir is the same as https://huggingface.co/transformers/serialization.html#cache-directory .
#     cache_dir: null
#   - name: "LanguageModelTokenizer"
#     # Flag to check whether to split intents
#     "intent_tokenization_flag": False
#     # Symbol on which intent should be split
#     "intent_split_symbol": "_"
#   - name: "LanguageModelFeaturizer"
#   - name: RegexFeaturizer
#   - name: LexicalSyntacticFeaturizer
#   - name: CountVectorsFeaturizer
#   - name: CountVectorsFeaturizer
#     analyzer: char_wb
#     min_ngram: 1
#     max_ngram: 4
#   # - name: "KeywordIntentClassifier"
#   - name: DIETClassifier
#     epochs: 100
#   - name: "CRFEntityExtractor"
#     # BILOU_flag determines whether to use BILOU tagging or not.
#     "BILOU_flag": True
#     # features to extract in the sliding window
#     "features": [
#       ["low", "title", "upper"],
#       [
#         "bias",
#         "low",
#         "prefix5",
#         "prefix2",
#         "suffix5",
#         "suffix3",
#         "suffix2",
#         "upper",
#         "title",
#         "digit",
#         "pattern",
#       ],
#       ["low", "title", "upper"],
#     ]
#     # The maximum number of iterations for optimization algorithms.
#     "max_iterations": 50
#     # weight of the L1 regularization
#     "L1_c": 0.1
#     # weight of the L2 regularization
#     "L2_c": 0.1
#     # Name of dense featurizers to use.
#     # If list is empty all available dense features are used.
#     "featurizers": []
#   - name: RegexEntityExtractor
#   - name: FallbackClassifier
#     threshold: 0.3
#     ambiguity_threshold: 0.1

    # pipeline:
#   - name: HFTransformersNLP
#     # Name of the language model to use
#     model_name: "bert"
#     # Pre-Trained weights to be loaded
#     model_weights: "bert-base-uncased"
#     # An optional path to a specific directory to download and cache the pre-trained model weights.
#     # The `default` cache_dir is the same as https://huggingface.co/transformers/serialization.html#cache-directory .
#     cache_dir: null
#   - name: "LanguageModelTokenizer"
#     # Flag to check whether to split intents
#     "intent_tokenization_flag": False
#     # Symbol on which intent should be split
#     "intent_split_symbol": "_"
#   - name: "LanguageModelFeaturizer"
#   - name: RegexFeaturizer
#   - name: LexicalSyntacticFeaturizer
#   - name: CountVectorsFeaturizer
#   - name: CountVectorsFeaturizer
#     analyzer: char_wb
#     min_ngram: 1
#     max_ngram: 4
#   # - name: "KeywordIntentClassifier"
#   - name: DIETClassifier
#     epochs: 100
#   - name: "CRFEntityExtractor"
#     # BILOU_flag determines whether to use BILOU tagging or not.
#     "BILOU_flag": True
#     # features to extract in the sliding window
#     "features": [
#       ["low", "title", "upper"],
#       [
#         "bias",
#         "low",
#         "prefix5",
#         "prefix2",
#         "suffix5",
#         "suffix3",
#         "suffix2",
#         "upper",
#         "title",
#         "digit",
#         "pattern",
#       ],
#       ["low", "title", "upper"],
#     ]
#     # The maximum number of iterations for optimization algorithms.
#     "max_iterations": 50
#     # weight of the L1 regularization
#     "L1_c": 0.1
#     # weight of the L2 regularization
#     "L2_c": 0.1
#     # Name of dense featurizers to use.
#     # If list is empty all available dense features are used.
#     "featurizers": []
#   - name: RegexEntityExtractor
#   - name: FallbackClassifier
#     threshold: 0.1
#     ambiguity_threshold: 0.05


# pipeline:
#   - name: "ConveRTTokenizer"
#     # Flag to check whether to split intents
#     "intent_tokenization_flag": False
#     # Symbol on which intent should be split
#     "intent_split_symbol": "_"
#     # Regular expression to detect tokens
#     "token_pattern": None
#     # Remote URL of hosted model
#     "model_url": TF_HUB_MODULE_URL
#   - name: "ConveRTFeaturizer"
#   - name: RegexFeaturizer
#   - name: LexicalSyntacticFeaturizer
#   - name: CountVectorsFeaturizer
#   - name: CountVectorsFeaturizer
#     analyzer: char_wb
#     min_ngram: 1
#     max_ngram: 4
#   - name: "KeywordIntentClassifier"
#   - name: "CRFEntityExtractor"
#     # BILOU_flag determines whether to use BILOU tagging or not.
#     "BILOU_flag": True
#     # features to extract in the sliding window
#     "features": [
#       ["low", "title", "upper"],
#       [
#         "bias",
#         "low",
#         "prefix5",
#         "prefix2",
#         "suffix5",
#         "suffix3",
#         "suffix2",
#         "upper",
#         "title",
#         "digit",
#         "pattern",
#       ],
#       ["low", "title", "upper"],
#     ]
#     # The maximum number of iterations for optimization algorithms.
#     "max_iterations": 50
#     # weight of the L1 regularization
#     "L1_c": 0.1
#     # weight of the L2 regularization
#     "L2_c": 0.1
#     # Name of dense featurizers to use.
#     # If list is empty all available dense features are used.
#     "featurizers": []
#   - name: RegexEntityExtractor
#   - name: FallbackClassifier
#     threshold: 0.3
#     ambiguity_threshold: 0.1


# pipeline:
# # No configuration for the NLU pipeline was provided. The following default pipeline was used to train your model.
# # If you'd like to customize it, uncomment and adjust the pipeline.
# # See https://rasa.com/docs/rasa/tuning-your-model for more information.
#   - name: WhitespaceTokenizer
#     "intent_tokenization_flag": True
#     "intent_split_symbol": "+"
#   - name: RegexFeaturizer
#   - name: LexicalSyntacticFeaturizer
#   - name: CountVectorsFeaturizer
#   - name: CountVectorsFeaturizer
#     analyzer: char_wb
#     min_ngram: 1
#     max_ngram: 4
#   - name: DIETClassifier
#     epochs: 50
#   - name: EntitySynonymMapper
#   - name: ResponseSelector
#     epochs: 100
#   - name: FallbackClassifier
#     threshold: 0.3
#     ambiguity_threshold: 0.1

# Configuration for Rasa Core.
# https://rasa.com/docs/rasa/core/policies/
policies:
# # No configuration for policies was provided. The following default policies were used to train your model.
# # If you'd like to customize them, uncomment and adjust the policies.
# # See https://rasa.com/docs/rasa/policies for more information.
#   - name: MemoizationPolicy
#   - name: TEDPolicy
#     max_history: 5
#     epochs: 100
#   - name: RulePolicy

pipeline:
- name: "SpacyNLP"
  # language model to load
  model: "en_core_web_md"
- name: "SpacyTokenizer"
  # Flag to check whether to split intents
  "intent_tokenization_flag": True
  # Symbol on which intent should be split
  "intent_split_symbol": "+"
  # Regular expression to detect tokens
  "token_pattern": None
- name: "SpacyFeaturizer"
  # Specify what pooling operation should be used to calculate the vector of
  # the complete utterance. Available options: 'mean' and 'max'.
  "pooling": "mean"
- name: "SklearnIntentClassifier"
  # Specifies the list of regularization values to
  # cross-validate over for C-SVM.
  # This is used with the ``kernel`` hyperparameter in GridSearchCV.
  C: [1, 2, 5, 10, 20, 100]
  # Specifies the kernel to use with C-SVM.
  # This is used with the ``C`` hyperparameter in GridSearchCV.
  kernels: ["linear"]
  # Gamma parameter of the C-SVM.
  "gamma": [0.1]
  # We try to find a good number of cross folds to use during
  # intent training, this specifies the max number of folds.
  "max_cross_validation_folds": 5
  # Scoring function used for evaluating the hyper parameters.
  # This can be a name or a function.
  "scoring_function": "f1_weighted"
- name: "SpacyEntityExtractor"
  # dimensions to extract
  dimensions: ["PERSON", "LOC", "ORG", "PRODUCT"]
- name: "DucklingEntityExtractor"
  # url of the running duckling server
  url: "http://localhost:8000"
  # dimensions to extract
  dimensions: ["time", "number", "amount-of-money", "distance"]
  # allows you to configure the locale, by default the language is
  # used
  locale: "de_DE"
  # if not set the default timezone of Duckling is going to be used
  # needed to calculate dates from relative expressions like "tomorrow"
  timezone: "Europe/Berlin"
  # Timeout for receiving response from http url of the running duckling server
  # if not set the default timeout of duckling http url is set to 3 seconds.
  timeout : 3
- name: RegexEntityExtractor
  # text will be processed with case insensitive as default
  case_sensitive: False
  # use lookup tables to extract entities
  use_lookup_tables: True
  # use regexes to extract entities
  use_regexes: True
- name: "EntitySynonymMapper"
- name: ResponseSelector
  epochs: 100
- name: FallbackClassifier
  threshold: 0.3
  ambiguity_threshold: 0.1

# pipeline:
# - name: "SpacyNLP"
#   # language model to load
#   model: "en_core_web_md"
# - name: "SpacyTokenizer"
#   # Flag to check whether to split intents
#   "intent_tokenization_flag": True
#   # Symbol on which intent should be split
#   "intent_split_symbol": "+"
#   # Regular expression to detect tokens
#   "token_pattern": None
# - name: "SpacyFeaturizer"
#   # Specify what pooling operation should be used to calculate the vector of
#   # the complete utterance. Available options: 'mean' and 'max'.
#   "pooling": "mean"
# - name: "SklearnIntentClassifier"
#   # Specifies the list of regularization values to
#   # cross-validate over for C-SVM.
#   # This is used with the ``kernel`` hyperparameter in GridSearchCV.
#   C: [1, 2, 5, 10, 20, 100]
#   # Specifies the kernel to use with C-SVM.
#   # This is used with the ``C`` hyperparameter in GridSearchCV.
#   kernels: ["linear"]
#   # Gamma parameter of the C-SVM.
#   "gamma": [0.1]
#   # We try to find a good number of cross folds to use during
#   # intent training, this specifies the max number of folds.
#   "max_cross_validation_folds": 5
#   # Scoring function used for evaluating the hyper parameters.
#   # This can be a name or a function.
#   "scoring_function": "f1_weighted"
# - name: "SpacyEntityExtractor"
#   # dimensions to extract
#   dimensions: ["actress", "date", "moviename", "mpaa_rating","genre","distanceconstraints","city","video_format", "numberofpeople","theater_chain","actor","theater,"numberofkids","starttime","state","movie_series","description","zip"]
# - name: "DucklingEntityExtractor"
#   # url of the running duckling server
#   url: "http://localhost:8000"
#   # dimensions to extract
#   dimensions: ["time", "number", "amount-of-money", "distance"]
#   # allows you to configure the locale, by default the language is
#   # used
#   locale: "de_DE"
#   # if not set the default timezone of Duckling is going to be used
#   # needed to calculate dates from relative expressions like "tomorrow"
#   timezone: "Europe/Berlin"
#   # Timeout for receiving response from http url of the running duckling server
#   # if not set the default timeout of duckling http url is set to 3 seconds.
#   timeout : 3
# - name: RegexEntityExtractor
#   # text will be processed with case insensitive as default
#   case_sensitive: False
#   # use lookup tables to extract entities
#   use_lookup_tables: True
#   # use regexes to extract entities
#   use_regexes: True
# - name: "EntitySynonymMapper"
# - name: ResponseSelector
#   epochs: 100
# - name: FallbackClassifier
#   threshold: 0.3
#   ambiguity_threshold: 0.1


# pipeline:
# - name: "MitieNLP"
#   # language model to load
#   model: "data/total_word_feature_extractor.dat"
# - name: "MitieTokenizer"
#   # Flag to check whether to split intents
#   "intent_tokenization_flag": True
#   # Symbol on which intent should be split
#   "intent_split_symbol": "+"
#   # Regular expression to detect tokens
#   "token_pattern": None
# - name: "MitieFeaturizer"
#   # Specify what pooling operation should be used to calculate the vector of
#   # the complete utterance. Available options: 'mean' and 'max'.
#   "pooling": "mean"
# - name: "MitieIntentClassifier"
# - name: "MitieEntityExtractor"
# - name: RegexEntityExtractor
#   # text will be processed with case insensitive as default
#   case_sensitive: False
#   # use lookup tables to extract entities
#   use_lookup_tables: True
#   # use regexes to extract entities
#   use_regexes: True
# - name: "EntitySynonymMapper"


# pipeline:
#   - name: HFTransformersNLP
#     # Name of the language model to use
#     model_name: "bert"
#     # Pre-Trained weights to be loaded
#     model_weights: "bert-base-uncased"
#     # An optional path to a specific directory to download and cache the pre-trained model weights.
#     # The `default` cache_dir is the same as https://huggingface.co/transformers/serialization.html#cache-directory .
#     cache_dir: null
#   - name: "SpacyNLP"
#   # language model to load
#     model: "en_core_web_md"
#   - name: "LanguageModelTokenizer"
#     # Flag to check whether to split intents
#     "intent_tokenization_flag": False
#     # Symbol on which intent should be split
#     "intent_split_symbol": "_"
#   - name: "SpacyTokenizer"
#     # Flag to check whether to split intents
#     "intent_tokenization_flag": True
#     # Symbol on which intent should be split
#     "intent_split_symbol": "+"
#     # Regular expression to detect tokens
#     "token_pattern": None
#   - name: "LanguageModelFeaturizer"
#   - name: "SpacyFeaturizer"
#   # Specify what pooling operation should be used to calculate the vector of
#   # the complete utterance. Available options: 'mean' and 'max'.
#     "pooling": "mean"
#   - name: RegexFeaturizer
#   - name: LexicalSyntacticFeaturizer
#   - name: CountVectorsFeaturizer
#   - name: CountVectorsFeaturizer
#     analyzer: char_wb
#     min_ngram: 1
#     max_ngram: 4
#   # - name: "KeywordIntentClassifier"
#   - name: DIETClassifier
#     epochs: 100
#   # - name: "SklearnIntentClassifier"
#   #   # Specifies the list of regularization values to
#   #   # cross-validate over for C-SVM.
#   #   # This is used with the ``kernel`` hyperparameter in GridSearchCV.
#   #   C: [1, 2, 5, 10, 20, 100]
#   #   # Specifies the kernel to use with C-SVM.
#   #   # This is used with the ``C`` hyperparameter in GridSearchCV.
#   #   kernels: ["linear"]
#   #   # Gamma parameter of the C-SVM.
#   #   "gamma": [0.1]
#   #   # We try to find a good number of cross folds to use during
#   #   # intent training, this specifies the max number of folds.
#   #   "max_cross_validation_folds": 5
#   #   # Scoring function used for evaluating the hyper parameters.
#   #   # This can be a name or a function.
#   #   "scoring_function": "f1_weighted"
#   - name: "SpacyEntityExtractor"
#     # dimensions to extract
#     dimensions: ["PERSON", "LOC", "ORG", "PRODUCT"]
#   - name: "CRFEntityExtractor"
#     # BILOU_flag determines whether to use BILOU tagging or not.
#     "BILOU_flag": True
#     # features to extract in the sliding window
#     "features": [
#       ["low", "title", "upper"],
#       [
#         "bias",
#         "low",
#         "prefix5",
#         "prefix2",
#         "suffix5",
#         "suffix3",
#         "suffix2",
#         "upper",
#         "title",
#         "digit",
#         "pattern",
#       ],
#       ["low", "title", "upper"],
#     ]
#     # The maximum number of iterations for optimization algorithms.
#     "max_iterations": 50
#     # weight of the L1 regularization
#     "L1_c": 0.1
#     # weight of the L2 regularization
#     "L2_c": 0.1
#     # Name of dense featurizers to use.
#     # If list is empty all available dense features are used.
#     "featurizers": []
#   - name: RegexEntityExtractor
#   - name: FallbackClassifier
#     threshold: 0.3
#     ambiguity_threshold: 0.1