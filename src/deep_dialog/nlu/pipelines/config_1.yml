# Configuration for Rasa NLU.
# https://rasa.com/docs/rasa/nlu/components/
language: en

pipeline:
  - name: HFTransformersNLP
    # Name of the language model to use
    model_name: "bert"
    # Pre-Trained weights to be loaded
    model_weights: "bert-base-uncased"
    # An optional path to a specific directory to download and cache the pre-trained model weights.
    # The `default` cache_dir is the same as https://huggingface.co/transformers/serialization.html#cache-directory .
    cache_dir: null
  - name: "LanguageModelTokenizer"
    # Flag to check whether to split intents
    "intent_tokenization_flag": False
    # Symbol on which intent should be split
    "intent_split_symbol": "_"
  - name: "LanguageModelFeaturizer"
  - name: RegexFeaturizer
  - name: LexicalSyntacticFeaturizer
  - name: CountVectorsFeaturizer
  - name: CountVectorsFeaturizer
    analyzer: char_wb
    min_ngram: 1
    max_ngram: 4
  # - name: "KeywordIntentClassifier"
  - name: DIETClassifier
    epochs: 100
  # - name: "CRFEntityExtractor"
  #   # BILOU_flag determines whether to use BILOU tagging or not.
  #   "BILOU_flag": True
  #   # features to extract in the sliding window
  #   "features": [
  #     ["low", "title", "upper"],
  #     [
  #       "bias",
  #       "low",
  #       "prefix5",
  #       "prefix2",
  #       "suffix5",
  #       "suffix3",
  #       "suffix2",
  #       "upper",
  #       "title",
  #       "digit",
  #       "pattern",
  #     ],
  #     ["low", "title", "upper"],
  #   ]
  #   # The maximum number of iterations for optimization algorithms.
  #   "max_iterations": 50
  #   # weight of the L1 regularization
  #   "L1_c": 0.1
  #   # weight of the L2 regularization
  #   "L2_c": 0.1
    # Name of dense featurizers to use.
    # If list is empty all available dense features are used.
    # "featurizers": []
  - name: RegexEntityExtractor
  - name: FallbackClassifier
    threshold: 0.3
    ambiguity_threshold: 0.1


# pipeline:
#   - name: "ConveRTTokenizer"
#     # Flag to check whether to split intents
#     "intent_tokenization_flag": False
#     # Symbol on which intent should be split
#     "intent_split_symbol": "_"
#     # Regular expression to detect tokens
#     "token_pattern": None
#     # Remote URL of hosted model
#     "model_url": TF_HUB_MODULE_URL
#   - name: "ConveRTFeaturizer"
#   - name: RegexFeaturizer
#   - name: LexicalSyntacticFeaturizer
#   - name: CountVectorsFeaturizer
#   - name: CountVectorsFeaturizer
#     analyzer: char_wb
#     min_ngram: 1
#     max_ngram: 4
#   - name: "KeywordIntentClassifier"
#   - name: "CRFEntityExtractor"
#     # BILOU_flag determines whether to use BILOU tagging or not.
#     "BILOU_flag": True
#     # features to extract in the sliding window
#     "features": [
#       ["low", "title", "upper"],
#       [
#         "bias",
#         "low",
#         "prefix5",
#         "prefix2",
#         "suffix5",
#         "suffix3",
#         "suffix2",
#         "upper",
#         "title",
#         "digit",
#         "pattern",
#       ],
#       ["low", "title", "upper"],
#     ]
#     # The maximum number of iterations for optimization algorithms.
#     "max_iterations": 50
#     # weight of the L1 regularization
#     "L1_c": 0.1
#     # weight of the L2 regularization
#     "L2_c": 0.1
#     # Name of dense featurizers to use.
#     # If list is empty all available dense features are used.
#     "featurizers": []
#   - name: RegexEntityExtractor
#   - name: FallbackClassifier
#     threshold: 0.3
#     ambiguity_threshold: 0.1


# pipeline:
# # No configuration for the NLU pipeline was provided. The following default pipeline was used to train your model.
# # If you'd like to customize it, uncomment and adjust the pipeline.
# # See https://rasa.com/docs/rasa/tuning-your-model for more information.
#   - name: WhitespaceTokenizer
#     "intent_tokenization_flag": True
#     "intent_split_symbol": "+"
#   - name: RegexFeaturizer
#   - name: LexicalSyntacticFeaturizer
#   - name: CountVectorsFeaturizer
#   - name: CountVectorsFeaturizer
#     analyzer: char_wb
#     min_ngram: 1
#     max_ngram: 4
#   - name: DIETClassifier
#     epochs: 50
#   - name: EntitySynonymMapper
#   - name: ResponseSelector
#     epochs: 100
#   - name: FallbackClassifier
#     threshold: 0.1
#     ambiguity_threshold: 0.05

# Configuration for Rasa Core.
# https://rasa.com/docs/rasa/core/policies/
policies:
# # No configuration for policies was provided. The following default policies were used to train your model.
# # If you'd like to customize them, uncomment and adjust the policies.
# # See https://rasa.com/docs/rasa/policies for more information.
#   - name: MemoizationPolicy
#   - name: TEDPolicy
#     max_history: 5
#     epochs: 100
#   - name: RulePolicy

# pipeline:
# - name: "SpacyNLP"
#   # language model to load
#   model: "en_core_web_md"
# - name: "SpacyTokenizer"
#   # Flag to check whether to split intents
#   "intent_tokenization_flag": True
#   # Symbol on which intent should be split
#   "intent_split_symbol": "+"
#   # Regular expression to detect tokens
#   "token_pattern": None
# - name: "SpacyFeaturizer"
#   # Specify what pooling operation should be used to calculate the vector of
#   # the complete utterance. Available options: 'mean' and 'max'.
#   "pooling": "mean"
# - name: "SklearnIntentClassifier"
#   # Specifies the list of regularization values to
#   # cross-validate over for C-SVM.
#   # This is used with the ``kernel`` hyperparameter in GridSearchCV.
#   C: [1, 2, 5, 10, 20, 100]
#   # Specifies the kernel to use with C-SVM.
#   # This is used with the ``C`` hyperparameter in GridSearchCV.
#   kernels: ["linear"]
#   # Gamma parameter of the C-SVM.
#   "gamma": [0.1]
#   # We try to find a good number of cross folds to use during
#   # intent training, this specifies the max number of folds.
#   "max_cross_validation_folds": 5
#   # Scoring function used for evaluating the hyper parameters.
#   # This can be a name or a function.
#   "scoring_function": "f1_weighted"
# - name: "SpacyEntityExtractor"
#   # dimensions to extract
#   dimensions: ["PERSON", "LOC", "ORG", "PRODUCT"]
# - name: "DucklingEntityExtractor"
#   # url of the running duckling server
#   url: "http://localhost:8000"
#   # dimensions to extract
#   dimensions: ["time", "number", "amount-of-money", "distance"]
#   # allows you to configure the locale, by default the language is
#   # used
#   locale: "de_DE"
#   # if not set the default timezone of Duckling is going to be used
#   # needed to calculate dates from relative expressions like "tomorrow"
#   timezone: "Europe/Berlin"
#   # Timeout for receiving response from http url of the running duckling server
#   # if not set the default timeout of duckling http url is set to 3 seconds.
#   timeout : 3
# - name: RegexEntityExtractor
#   # text will be processed with case insensitive as default
#   case_sensitive: False
#   # use lookup tables to extract entities
#   use_lookup_tables: True
#   # use regexes to extract entities
#   use_regexes: True
# - name: "EntitySynonymMapper"
# - name: ResponseSelector
#   epochs: 100
# - name: FallbackClassifier
#   threshold: 0.3
#   ambiguity_threshold: 0.1


# pipeline:
# - name: "MitieNLP"
#   # language model to load
#   model: "data/total_word_feature_extractor.dat"
# - name: "MitieTokenizer"
#   # Flag to check whether to split intents
#   "intent_tokenization_flag": True
#   # Symbol on which intent should be split
#   "intent_split_symbol": "+"
#   # Regular expression to detect tokens
#   "token_pattern": None
# - name: "MitieFeaturizer"
#   # Specify what pooling operation should be used to calculate the vector of
#   # the complete utterance. Available options: 'mean' and 'max'.
#   "pooling": "mean"
# - name: "MitieIntentClassifier"
# - name: "MitieEntityExtractor"
# - name: RegexEntityExtractor
#   # text will be processed with case insensitive as default
#   case_sensitive: False
#   # use lookup tables to extract entities
#   use_lookup_tables: True
#   # use regexes to extract entities
#   use_regexes: True
# - name: "EntitySynonymMapper"


# pipeline:
#   - name: HFTransformersNLP
#     # Name of the language model to use
#     model_name: "bert"
#     # Pre-Trained weights to be loaded
#     model_weights: "bert-base-uncased"
#     # An optional path to a specific directory to download and cache the pre-trained model weights.
#     # The `default` cache_dir is the same as https://huggingface.co/transformers/serialization.html#cache-directory .
#     cache_dir: null
#   - name: "SpacyNLP"
#   # language model to load
#     model: "en_core_web_md"
#   - name: "LanguageModelTokenizer"
#     # Flag to check whether to split intents
#     "intent_tokenization_flag": False
#     # Symbol on which intent should be split
#     "intent_split_symbol": "_"
#   - name: "SpacyTokenizer"
#     # Flag to check whether to split intents
#     "intent_tokenization_flag": True
#     # Symbol on which intent should be split
#     "intent_split_symbol": "+"
#     # Regular expression to detect tokens
#     "token_pattern": None
#   - name: "LanguageModelFeaturizer"
#   - name: "SpacyFeaturizer"
#   # Specify what pooling operation should be used to calculate the vector of
#   # the complete utterance. Available options: 'mean' and 'max'.
#     "pooling": "mean"
#   - name: RegexFeaturizer
#   - name: LexicalSyntacticFeaturizer
#   - name: CountVectorsFeaturizer
#   - name: CountVectorsFeaturizer
#     analyzer: char_wb
#     min_ngram: 1
#     max_ngram: 4
#   # - name: "KeywordIntentClassifier"
#   - name: DIETClassifier
#     epochs: 100
#   # - name: "SklearnIntentClassifier"
#   #   # Specifies the list of regularization values to
#   #   # cross-validate over for C-SVM.
#   #   # This is used with the ``kernel`` hyperparameter in GridSearchCV.
#   #   C: [1, 2, 5, 10, 20, 100]
#   #   # Specifies the kernel to use with C-SVM.
#   #   # This is used with the ``C`` hyperparameter in GridSearchCV.
#   #   kernels: ["linear"]
#   #   # Gamma parameter of the C-SVM.
#   #   "gamma": [0.1]
#   #   # We try to find a good number of cross folds to use during
#   #   # intent training, this specifies the max number of folds.
#   #   "max_cross_validation_folds": 5
#   #   # Scoring function used for evaluating the hyper parameters.
#   #   # This can be a name or a function.
#   #   "scoring_function": "f1_weighted"
#   - name: "SpacyEntityExtractor"
#     # dimensions to extract
#     dimensions: ["PERSON", "LOC", "ORG", "PRODUCT"]
#   - name: "CRFEntityExtractor"
#     # BILOU_flag determines whether to use BILOU tagging or not.
#     "BILOU_flag": True
#     # features to extract in the sliding window
#     "features": [
#       ["low", "title", "upper"],
#       [
#         "bias",
#         "low",
#         "prefix5",
#         "prefix2",
#         "suffix5",
#         "suffix3",
#         "suffix2",
#         "upper",
#         "title",
#         "digit",
#         "pattern",
#       ],
#       ["low", "title", "upper"],
#     ]
#     # The maximum number of iterations for optimization algorithms.
#     "max_iterations": 50
#     # weight of the L1 regularization
#     "L1_c": 0.1
#     # weight of the L2 regularization
#     "L2_c": 0.1
#     # Name of dense featurizers to use.
#     # If list is empty all available dense features are used.
#     "featurizers": []
#   - name: RegexEntityExtractor
#   - name: FallbackClassifier
#     threshold: 0.3
#     ambiguity_threshold: 0.1